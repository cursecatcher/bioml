import numpy as np 
import pandas as pd 
from sklearn.base import BaseEstimator
from sklearn.metrics import cohen_kappa_score

from .rule import Rule, replace_feature_name
from .skope_rules import SkopeRules 


class DualSkoper(BaseEstimator):
    """An easy-interpretable classifier optimizing simple logical rules.

    Parameters
    ----------

    feature_names : list of str, optional
        The names of each feature to be used for returning rules in string
        format.

    precision_min : float, optional (default=0.5)
        The minimal precision of a rule to be selected.

    recall_min : float, optional (default=0.01)
        The minimal recall of a rule to be selected.

    n_estimators : int, optional (default=10)
        The number of base estimators (rules) to use for prediction. More are
        built before selection. All are available in the estimators_ attribute.

    max_samples : int or float, optional (default=.8)
        The number of samples to draw from X to train each decision tree, from
        which rules are generated and selected.
            - If int, then draw `max_samples` samples.
            - If float, then draw `max_samples * X.shape[0]` samples.
        If max_samples is larger than the number of samples provided,
        all samples will be used for all trees (no sampling).

    max_samples_features : int or float, optional (default=1.0)
        The number of features to draw from X to train each decision tree, from
        which rules are generated and selected.
            - If int, then draw `max_features` features.
            - If float, then draw `max_features * X.shape[1]` features.

    bootstrap : boolean, optional (default=False)
        Whether samples are drawn with replacement.

    bootstrap_features : boolean, optional (default=False)
        Whether features are drawn with replacement.

    max_depth : integer or List or None, optional (default=3)
        The maximum depth of the decision trees. If None, then nodes are
        expanded until all leaves are pure or until all leaves contain less
        than min_samples_split samples.
        If an iterable is passed, you will train n_estimators
        for each tree depth. It allows you to create and compare
        rules of different length.

    max_depth_duplication : integer, optional (default=None)
        The maximum depth of the decision tree for rule deduplication,
        if None then no deduplication occurs.

    max_features : int, float, string or None, optional (default="auto")
        The number of features considered (by each decision tree) when looking
        for the best split:

        - If int, then consider `max_features` features at each split.
        - If float, then `max_features` is a percentage and
          `int(max_features * n_features)` features are considered at each
          split.
        - If "auto", then `max_features=sqrt(n_features)`.
        - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
        - If "log2", then `max_features=log2(n_features)`.
        - If None, then `max_features=n_features`.

        Note: the search for a split does not stop until at least one
        valid partition of the node samples is found, even if it requires to
        effectively inspect more than ``max_features`` features.

    min_samples_split : int, float, optional (default=2)
        The minimum number of samples required to split an internal node for
        each decision tree.
            - If int, then consider `min_samples_split` as the minimum number.
            - If float, then `min_samples_split` is a percentage and
              `ceil(min_samples_split * n_samples)` are the minimum
              number of samples for each split.

    n_jobs : integer, optional (default=1)
        The number of jobs to run in parallel for both `fit` and `predict`.
        If -1, then the number of jobs is set to the number of cores.

    random_state : int, RandomState instance or None, optional
        - If int, random_state is the seed used by the random number generator.
        - If RandomState instance, random_state is the random number generator.
        - If None, the random number generator is the RandomState instance used
        by `np.random`.

    verbose : int, optional (default=0)
        Controls the verbosity of the tree building process."""

    # Attributes
    # ----------
    # rules_ : dict of tuples (rule, precision, recall, nb).
    #     The collection of `n_estimators` rules used in the ``predict`` method.
    #     The rules are generated by fitted sub-estimators (decision trees). Each
    #     rule satisfies recall_min and precision_min conditions. The selection
    #     is done according to OOB precisions.

    # estimators_ : list of DecisionTreeClassifier
    #     The collection of fitted sub-estimators used to generate candidate
    #     rules.

    # estimators_samples_ : list of arrays
    #     The subset of drawn samples (i.e., the in-bag samples) for each base
    #     estimator.

    # estimators_features_ : list of arrays
    #     The subset of drawn features for each base estimator.

    # max_samples_ : integer
    #     The actual number of samples

    # n_features_ : integer
    #     The number of features when ``fit`` is performed.

    # classes_ : array, shape (n_classes,)
    #     The classes labels.
    # """
    def __init__(self,
                 feature_names=None,
                 precision_min=0.5,
                 recall_min=0.01,
                 n_estimators=10,
                 max_samples=.8,
                 max_samples_features=1.,
                 bootstrap=False,
                 bootstrap_features=False,
                 max_depth=3,
                 max_depth_duplication=None,
                 max_features=1.,
                 min_samples_split=2,
                 n_jobs=1,
                 random_state=None,
                 verbose=0):
        
        self.precision_min = precision_min
        self.recall_min = recall_min
        self.feature_names = feature_names
        self.n_estimators = n_estimators
        self.max_samples = max_samples
        self.max_samples_features = max_samples_features
        self.bootstrap = bootstrap
        self.bootstrap_features = bootstrap_features
        self.max_depth = max_depth
        self.max_depth_duplication = max_depth_duplication
        self.max_features = max_features
        self.min_samples_split = min_samples_split
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.verbose = verbose
    
    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.

        y : array-like, shape (n_samples,)
            Target vector relative to X. Has to follow the convention 0 for
            normal data, 1 for anomalies.

        sample_weight : array-like, shape (n_samples,) optional
            Array of weights that are assigned to individual samples, typically
            the amount in case of transactions data. Used to grow regression
            trees producing further rules to be tested.
            If not provided, then each sample is given unit weight.

        Returns
        -------
        self : object
            Returns self.
        """

        args = dict(
            feature_names=self.feature_names, 
            precision_min=self.precision_min, recall_min=self.recall_min, 
            n_estimators=self.n_estimators, 
            max_samples=self.max_samples, max_samples_features=self.max_samples_features, 
            bootstrap=self.bootstrap, bootstrap_features=self.bootstrap_features, 
            max_depth=self.max_depth, max_depth_duplication=self.max_depth_duplication, 
            max_features=self.max_features, min_samples_split=self.min_samples_split, 
            n_jobs=self.n_jobs, random_state=self.random_state, verbose=self.verbose)

        #fit two classifiers, so that the first one learns rules for the positive class
        #while the second one learns rules for the negative class 
        y_ = np.array(y, dtype=bool)
        self.__first = SkopeRules(**args).fit(X, y_, sample_weight=sample_weight)
        self.__second = SkopeRules(**args).fit(X, np.logical_not(y_), sample_weight=sample_weight)

        self.rules_ = (
            tuple(self.__first.rules_), 
            tuple(self.__second.rules_))
        self.rules_without_feature_names_ = (
            tuple(self.__first.rules_without_feature_names_), 
            tuple(self.__second.rules_without_feature_names_))

        return self 

    def predict(self, X):
        """Predict if a particular sample is an outlier or not.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input samples. Internally, it will be converted to
            ``dtype=np.float32``

        Returns
        -------
        is_outlier : array, shape (n_samples,)
            For each observations, tells whether or not (1 or 0) it should
            be considered as an outlier according to the selected rules.
        """

        return np.array([
            (v1 >= v2) for v1, v2 in zip(self.__first.decision_function(X), self.__second.decision_function(X))
        ], dtype=int)


    def score_top_rules(self, X):        
        return (
            self.__first.score_top_rules(X), 
            self.__second.score_top_rules(X)
        )


    def predict_top_rules(self, X, n_rules):
        """Predict if a particular sample is an outlier or not,
        using the n_rules most performing rules.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input samples. Internally, it will be converted to
            ``dtype=np.float32``

        n_rules : int
            The number of rules used for the prediction. If one of the
            n_rules most performing rules is activated, the prediction
            is equal to 1.

        Returns
        -------
        is_outlier : array, shape (n_samples,)
            For each observations, tells whether or not (1 or 0) it should
            be considered as an outlier according to the selected rules.
        """

        #score samples by f1-score 
        # first = self.__first.rules_vote_by_metric(X, n_rules)
        # second = self.__second.rules_vote_by_metric(X, n_rules)
        first = self.__first.rules_vote(X, n_rules)
        second = self.__second.rules_vote(X, n_rules)
        
        
        scores = (first - second) 

        return np.array([x >= 0 for x in scores], dtype=int)
    
 
    def get_ith_rule(self, i):
        """ """
        def get_ith(l, i):
            return l[i] if i < len(l) else None

        rules_pos, rules_neg = self.rules_
        return (get_ith(rules_pos, i), get_ith(rules_neg, i))


    # def rule_coverage(self, X, y):
    #     rules_pos, rules_neg = self.rules_

    #     #dataframe without index 
    #     df = pd.DataFrame(X.to_numpy(), columns = X.columns)

    #     rules_ensemble = DualSkoper.rules_perf(df, y, rules_pos, "+")
    #     rules_ensemble.update(DualSkoper.rules_perf(df, 1 - y, rules_neg, "-"))
    #     return rules_ensemble

